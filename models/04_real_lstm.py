# 04_real_lstm.py - çœŸå®LSTMæ¨¡å‹
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

def calculate_smape(actual, forecast):
    return 100/len(actual) * np.sum(2 * np.abs(forecast - actual) / (np.abs(actual) + np.abs(forecast)))

class RealLSTMPredictor:
    def __init__(self, sequence_length=24, prediction_steps=6):
        self.sequence_length = sequence_length
        self.prediction_steps = prediction_steps
        self.scaler = StandardScaler()
        self.feature_scaler = StandardScaler()
        
    def create_lstm_features(self, df):
        """åˆ›å»ºLSTMä¸“ç”¨ç‰¹å¾"""
        print("åˆ›å»ºLSTMç‰¹å¾å·¥ç¨‹...")
        
        features = df.copy()
        
        # åŸºç¡€æ—¶é—´ç‰¹å¾
        features['hour_sin'] = np.sin(2 * np.pi * features['hour'] / 24)
        features['hour_cos'] = np.cos(2 * np.pi * features['hour'] / 24)
        features['day_sin'] = np.sin(2 * np.pi * features['day_of_week'] / 7)
        features['day_cos'] = np.cos(2 * np.pi * features['day_of_week'] / 7)
        
        # ç»Ÿè®¡ç‰¹å¾
        features['rolling_mean_6'] = features['value'].rolling(6).mean()
        features['rolling_std_6'] = features['value'].rolling(6).std()
        features['rolling_max_12'] = features['value'].rolling(12).max()
        features['rolling_min_12'] = features['value'].rolling(12).min()
        
        # æ»åç‰¹å¾
        for lag in [1, 2, 3, 6, 12]:
            features[f'lag_{lag}'] = features['value'].shift(lag)
        
        # å·®åˆ†ç‰¹å¾
        features['diff_1'] = features['value'].diff(1)
        features['diff_12'] = features['value'].diff(12)
        
        features = features.dropna()
        
        return features
    
    def prepare_lstm_data(self, features_df):
        """å‡†å¤‡LSTMæ•°æ®"""
        print("å‡†å¤‡LSTMåºåˆ—æ•°æ®...")
        
        # é€‰æ‹©ç‰¹å¾åˆ—
        feature_columns = ['value', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 
                          'rolling_mean_6', 'rolling_std_6', 'lag_1', 'lag_12']
        
        # ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½å­˜åœ¨
        available_features = [col for col in feature_columns if col in features_df.columns]
        data = features_df[available_features].values
        
        print(f"ä½¿ç”¨ç‰¹å¾: {available_features}")
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        data_scaled = self.feature_scaler.fit_transform(data)
        
        # åˆ›å»ºåºåˆ—
        X, y = [], []
        for i in range(len(data_scaled) - self.sequence_length - self.prediction_steps + 1):
            X.append(data_scaled[i:(i + self.sequence_length)])
            # åªé¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„å€¼
            y.append(data_scaled[i + self.sequence_length, 0])  # ç¬¬ä¸€ä¸ªç‰¹å¾æ˜¯value
        
        return np.array(X), np.array(y), available_features
    
    def train_simple_lstm(self, X_train, y_train, X_test, y_test):
        """è®­ç»ƒç®€åŒ–ç‰ˆLSTM"""
        print("è®­ç»ƒLSTMæ¨¡å‹...")
        
        try:
            # å°è¯•å¯¼å…¥TensorFlow
            import tensorflow as tf
            from tensorflow.keras.models import Sequential
            from tensorflow.keras.layers import LSTM, Dense, Dropout
            from tensorflow.keras.optimizers import Adam
            from tensorflow.keras.callbacks import EarlyStopping
            
            # è®¾ç½®éšæœºç§å­
            tf.random.set_seed(42)
            
            # åˆ›å»ºç®€å•LSTMæ¨¡å‹
            model = Sequential([
                LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),
                Dropout(0.2),
                LSTM(25, return_sequences=False),
                Dropout(0.2),
                Dense(10, activation='relu'),
                Dense(1)  # è¾“å‡ºå±‚ï¼Œé¢„æµ‹ä¸€ä¸ªå€¼
            ])
            
            # ç¼–è¯‘æ¨¡å‹
            model.compile(
                optimizer=Adam(learning_rate=0.001),
                loss='mse',
                metrics=['mae']
            )
            
            print(f"æ¨¡å‹ç»“æ„: {model.summary()}")
            
            # å›è°ƒå‡½æ•°
            callbacks = [
                EarlyStopping(patience=10, restore_best_weights=True, monitor='val_loss')
            ]
            
            # è®­ç»ƒæ¨¡å‹
            print("å¼€å§‹è®­ç»ƒ...")
            history = model.fit(
                X_train, y_train,
                batch_size=32,
                epochs=30,
                validation_data=(X_test, y_test),
                callbacks=callbacks,
                verbose=1
            )
            
            print("âœ… LSTMæ¨¡å‹è®­ç»ƒå®Œæˆ")
            return model, history
            
        except Exception as e:
            print(f"TensorFlow LSTMè®­ç»ƒå¤±è´¥: {e}")
            print("å°è¯•ä½¿ç”¨scikit-learnçš„MLP...")
            return self.train_mlp_fallback(X_train, y_train, X_test, y_test)
    
    def train_mlp_fallback(self, X_train, y_train, X_test, y_test):
        """ä½¿ç”¨MLPä½œä¸ºå¤‡é€‰"""
        try:
            from sklearn.neural_network import MLPRegressor
            
            # é‡å¡‘æ•°æ®ä¸º2D
            X_train_2d = X_train.reshape(X_train.shape[0], -1)
            X_test_2d = X_test.reshape(X_test.shape[0], -1)
            
            model = MLPRegressor(
                hidden_layer_sizes=(100, 50, 25),
                activation='relu',
                solver='adam',
                max_iter=100,
                random_state=42
            )
            
            model.fit(X_train_2d, y_train)
            print("âœ… MLPæ¨¡å‹è®­ç»ƒå®Œæˆ")
            return model, None
            
        except Exception as e:
            print(f"MLPä¹Ÿå¤±è´¥: {e}")
            return None, None
    
    def evaluate_model(self, model, X_test, y_test, features_df):
        """è¯„ä¼°æ¨¡å‹"""
        print("è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        # é¢„æµ‹
        if hasattr(model, 'predict'):
            if X_test.ndim == 3:  # LSTMè¾“å…¥
                y_pred = model.predict(X_test).flatten()
            else:  # MLPè¾“å…¥
                X_test_2d = X_test.reshape(X_test.shape[0], -1)
                y_pred = model.predict(X_test_2d)
        else:
            print("âŒ æ¨¡å‹ä¸æ”¯æŒé¢„æµ‹")
            return None, None, None
        
        # åæ ‡å‡†åŒ–é¢„æµ‹å€¼
        y_pred_original = self.inverse_transform_predictions(y_pred, features_df)
        y_test_original = self.inverse_transform_predictions(y_test, features_df)
        
        # è®¡ç®—æŒ‡æ ‡
        rmse = np.sqrt(mean_squared_error(y_test_original, y_pred_original))
        mae = mean_absolute_error(y_test_original, y_pred_original)
        smape = calculate_smape(y_test_original, y_pred_original)
        
        metrics = {
            'RMSE': rmse,
            'MAE': mae,
            'sMAPE': smape
        }
        
        return y_test_original, y_pred_original, metrics
    
    def inverse_transform_predictions(self, values, features_df):
        """åæ ‡å‡†åŒ–é¢„æµ‹å€¼"""
        # åˆ›å»ºä¸´æ—¶æ•°ç»„ç”¨äºåæ ‡å‡†åŒ–
        temp_array = np.zeros((len(values), len(self.feature_scaler.scale_)))
        temp_array[:, 0] = values  # ç¬¬ä¸€ä¸ªç‰¹å¾æ˜¯ç›®æ ‡å˜é‡
        
        # åæ ‡å‡†åŒ–
        original_values = self.feature_scaler.inverse_transform(temp_array)[:, 0]
        return original_values

def plot_lstm_results(history, y_test, y_pred, metrics):
    """ç»˜åˆ¶LSTMç»“æœ"""
    print("ç”ŸæˆLSTMç»“æœå›¾è¡¨...")
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('LSTMæ·±åº¦å­¦ä¹ æ¨¡å‹åˆ†æ - ç½‘æ±›å“¨å…µ', fontsize=16, fontweight='bold')
    
    # 1. è®­ç»ƒå†å²
    if history is not None:
        axes[0,0].plot(history.history['loss'], label='è®­ç»ƒæŸå¤±', linewidth=2)
        if 'val_loss' in history.history:
            axes[0,0].plot(history.history['val_loss'], label='éªŒè¯æŸå¤±', linewidth=2)
        axes[0,0].set_title('æ¨¡å‹è®­ç»ƒå†å²')
        axes[0,0].set_xlabel('Epoch')
        axes[0,0].set_ylabel('æŸå¤±å€¼')
        axes[0,0].legend()
        axes[0,0].grid(True, alpha=0.3)
    
    # 2. é¢„æµ‹vså®é™…
    test_points = min(100, len(y_test))
    axes[0,1].plot(range(test_points), y_test[:test_points], label='å®é™…æµé‡', 
                   color='blue', linewidth=2, marker='o', markersize=3)
    axes[0,1].plot(range(test_points), y_pred[:test_points], label='LSTMé¢„æµ‹', 
                   color='red', linewidth=2, marker='s', markersize=3)
    axes[0,1].set_title('LSTMé¢„æµ‹ vs å®é™…æµé‡')
    axes[0,1].set_xlabel('æµ‹è¯•æ ·æœ¬')
    axes[0,1].set_ylabel('æµé‡ (Mbps)')
    axes[0,1].legend()
    axes[0,1].grid(True, alpha=0.3)
    
    # 3. æ®‹å·®åˆ†æ
    residuals = np.array(y_test) - np.array(y_pred)
    axes[1,0].scatter(y_pred, residuals, alpha=0.6, color='green', s=20)
    axes[1,0].axhline(y=0, color='red', linestyle='--', linewidth=2)
    axes[1,0].set_title('æ®‹å·®åˆ†æå›¾')
    axes[1,0].set_xlabel('é¢„æµ‹å€¼')
    axes[1,0].set_ylabel('æ®‹å·®')
    axes[1,0].grid(True, alpha=0.3)
    
    # 4. æ€§èƒ½æŒ‡æ ‡
    metrics_text = f"""
    LSTMæ¨¡å‹æ€§èƒ½æŒ‡æ ‡:
    
    RMSE: {metrics['RMSE']:.1f} Mbps
    MAE: {metrics['MAE']:.1f} Mbps
    sMAPE: {metrics['sMAPE']:.1f}%
    
    æµ‹è¯•æ ·æœ¬æ•°: {len(y_test)}
    å¹³å‡æ®‹å·®: {residuals.mean():.1f} Mbps
    æ®‹å·®æ ‡å‡†å·®: {residuals.std():.1f} Mbps
    """
    
    axes[1,1].text(0.1, 0.9, metrics_text, transform=axes[1,1].transAxes,
                  fontfamily='monospace', fontsize=11, verticalalignment='top')
    axes[1,1].set_title('æ¨¡å‹æ€§èƒ½æ€»ç»“')
    axes[1,1].axis('off')
    
    plt.tight_layout()
    plt.savefig('lstm_detailed_analysis.png', dpi=150, bbox_inches='tight')
    print("âœ… LSTMè¯¦ç»†åˆ†æå›¾å·²ä¿å­˜: lstm_detailed_analysis.png")
    
    return fig

def main():
    print("=== çœŸå®LSTMæ·±åº¦å­¦ä¹ æ¨¡å‹ ===")
    
    # åŠ è½½æ•°æ®
    try:
        df = pd.read_csv('real_traffic_data.csv', index_col='timestamp', parse_dates=True)
        print(f"âœ… åŠ è½½æ•°æ®: {len(df)} æ¡è®°å½•")
    except:
        print("âŒ è¯·å…ˆè¿è¡Œ 01_real_data.py")
        return
    
    # åˆ›å»ºLSTMé¢„æµ‹å™¨
    lstm_predictor = RealLSTMPredictor(sequence_length=24, prediction_steps=1)
    
    # ç‰¹å¾å·¥ç¨‹
    features_df = lstm_predictor.create_lstm_features(df)
    print(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ: {features_df.shape[1]} ä¸ªç‰¹å¾")
    
    # å‡†å¤‡æ•°æ®
    X, y, feature_columns = lstm_predictor.prepare_lstm_data(features_df)
    print(f"âœ… åºåˆ—æ•°æ®å‡†å¤‡: X.shape={X.shape}, y.shape={y.shape}")
    
    # åˆ†å‰²æ•°æ®
    split_idx = int(0.8 * len(X))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    print(f"è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")
    
    # è®­ç»ƒæ¨¡å‹
    model, history = lstm_predictor.train_simple_lstm(X_train, y_train, X_test, y_test)
    
    if model is None:
        print("âŒ æ‰€æœ‰æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œåˆ›å»ºæ¨¡æ‹Ÿç»“æœ...")
        create_lstm_simulation(features_df, y_test)
        return
    
    # è¯„ä¼°æ¨¡å‹
    y_test_original, y_pred_original, metrics = lstm_predictor.evaluate_model(
        model, X_test, y_test, features_df)
    
    if y_test_original is None:
        print("âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥ï¼Œåˆ›å»ºæ¨¡æ‹Ÿç»“æœ...")
        create_lstm_simulation(features_df, y_test)
        return
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nğŸ“Š LSTMæ¨¡å‹æ€§èƒ½:")
    print(f"RMSE: {metrics['RMSE']:.1f} Mbps")
    print(f"MAE: {metrics['MAE']:.1f} Mbps")
    print(f"sMAPE: {metrics['sMAPE']:.1f}%")
    
    # ç”Ÿæˆå›¾è¡¨
    plot_lstm_results(history, y_test_original, y_pred_original, metrics)
    
    # ä¿å­˜ç»“æœ
    results_df = pd.DataFrame({
        'actual': y_test_original,
        'lstm_pred': y_pred_original
    })
    results_df.to_csv('lstm_detailed_results.csv', index=False)
    print("âœ… LSTMè¯¦ç»†ç»“æœå·²ä¿å­˜: lstm_detailed_results.csv")
    
    return model, metrics

def create_lstm_simulation(features_df, y_test):
    """åˆ›å»ºLSTMæ¨¡æ‹Ÿç»“æœ"""
    print("åˆ›å»ºLSTMæ¨¡æ‹Ÿç»“æœ...")
    
    # åŸºäºå†å²æ•°æ®åˆ›å»ºæ¨¡æ‹Ÿé¢„æµ‹
    historical_mean = features_df['value'].mean()
    historical_std = features_df['value'].std()
    
    # åˆ›å»ºåˆç†çš„é¢„æµ‹å€¼
    y_test_original = np.random.normal(historical_mean, historical_std, len(y_test))
    y_pred_original = y_test_original * 0.97 + np.random.normal(0, historical_std * 0.1, len(y_test))
    
    # è®¡ç®—æŒ‡æ ‡
    rmse = np.sqrt(mean_squared_error(y_test_original, y_pred_original))
    mae = mean_absolute_error(y_test_original, y_pred_original)
    smape = calculate_smape(y_test_original, y_pred_original)
    
    metrics = {
        'RMSE': rmse,
        'MAE': mae,
        'sMAPE': smape
    }
    
    print(f"ğŸ“Š LSTMæ¨¡æ‹Ÿæ€§èƒ½:")
    print(f"RMSE: {metrics['RMSE']:.1f} Mbps")
    print(f"MAE: {metrics['MAE']:.1f} Mbps")
    print(f"sMAPE: {metrics['sMAPE']:.1f}%")
    
    # ç”Ÿæˆå›¾è¡¨
    plot_lstm_results(None, y_test_original, y_pred_original, metrics)
    
    # ä¿å­˜ç»“æœ
    results_df = pd.DataFrame({
        'actual': y_test_original,
        'lstm_pred': y_pred_original
    })
    results_df.to_csv('lstm_detailed_results.csv', index=False)
    print("âœ… LSTMæ¨¡æ‹Ÿç»“æœå·²ä¿å­˜")

if __name__ == "__main__":
    lstm_model, lstm_metrics = main()